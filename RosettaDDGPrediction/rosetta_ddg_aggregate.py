#!/usr/bin/env python
# -*- Mode: python; tab-width: 4; indent-tabs-mode:nil; coding:utf-8 -*-

#    rosetta_ddg_aggregate.py
#
#    Aggregate data generated by running Rosetta protocols for the
#    calculation of the ΔΔG of stability/binding upon mutation.
#
#    Copyright (C) 2020 Valentina Sora 
#                       <sora.valentina1@gmail.com>
#                       Matteo Tiberti 
#                       <matteo.tiberti@gmail.com> 
#                       Elena Papaleo
#                       <elenap@cancer.dk>
#
#    This program is free software: you can redistribute it and/or
#    modify it under the terms of the GNU General Public License as
#    published by the Free Software Foundation, either version 3 of
#    the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public
#    License along with this program. 
#    If not, see <http://www.gnu.org/licenses/>.



# standard libary
import argparse
import logging as log
import os
import os.path
import sys
# third-party packages
import dask
from distributed import Client, LocalCluster
import pandas as pd
# RosettaDDGProtocols
from . import aggregation 
from .defaults import (
    CONFIGAGGRDIR,
    CONFIGAGGRFILE,
    CONFIGRUNDIR,
    CONFIGSETTINGSDIR,
)
from . import util


def main():



    ######################### ARGUMENT PARSER #########################



    parser = argparse.ArgumentParser()

    cr_help = f"Configuration file of the protocol that was " \
              f"run. If it is a name without extension, it is " \
              f"assumed to be the name of a YAML file in " \
              f"{CONFIGRUNDIR}."
    parser.add_argument("-cr", "--configfile-run", \
                        type = str, \
                        required = True, \
                        help = cr_help)

    cs_help = \
        f"Configuration file containing settings to be used for " \
        f"the run. If it is a name without extension, it is assumed " \
        f"to be the name of a YAML file in {CONFIGSETTINGSDIR}. "
    parser.add_argument("-cs", "--configfile-settings", \
                        type = str, \
                        required = True, \
                        help = cs_help)

    ca_help = f"Configuration file for data aggregation. " \
              f"If it is a name without extension, it is assumed " \
              f"to be the name of a YAML file in {CONFIGAGGRDIR}. " \
              f"Default is {CONFIGAGGRFILE}."
    parser.add_argument("-ca", "--configfile-aggregate", \
                        type = str, \
                        default = CONFIGAGGRFILE, \
                        help = ca_help)

    d_help = "Directory where the protocol was run. " \
             "Default is the current working directory."
    parser.add_argument("-d", "--running-dir", \
                        type = str, \
                        default = os.getcwd(), \
                        help = d_help)

    od_help = \
        "Directory where to store the aggregated data. " \
        "Default is the current working directory."
    parser.add_argument("-od", "--output-dir", \
                        type = str, \
                        default = os.getcwd(), \
                        help = od_help)

    mf_help = "File with info about the mutations (it " \
              "is created when running)." 
    parser.add_argument("-mf", "--mutinfofile", \
                        type = str, \
                        help = mf_help)

    n_help = \
        "Number of processes to be started in parallel. " \
        "Default is one process (no parallelization)."
    parser.add_argument("-n", "--nproc", \
                        type = int, \
                        default = 1, \
                        help = n_help)

    # parse the arguments
    args = parser.parse_args()
    
    # configuration files
    configfilerun = args.configfile_run
    configfilesettings = args.configfile_settings
    configfileaggr = args.configfile_aggregate
    # directories
    rundir = args.running_dir
    outdir = args.output_dir
    # others
    mutinfofile = util.get_abspath(args.mutinfofile)
    nproc = args.nproc



    ############################## CLIENT #############################



    # try to get the run settings from the default YAML file
    try:
        settings = util.get_config_settings(configfilesettings)
    # if something went wrong, report it and exit
    except Exception as e:
        errstr = f"Could not parse the configuration file " \
                 f"{configfilesettings}: {e}"
        log.error(errstr)
        sys.exit(errstr)

    # create the local cluster
    cluster = LocalCluster(n_workers = nproc, \
                           **settings["localcluster"])
    
    # open the client from the cluster
    client = Client(cluster)



    ########################## CONFIGURATION ##########################



    # try to get the configuration of the run from the
    # corresponding configuration file
    try:
        configrun = util.get_config_run(configfilerun)
    # if something went wrong, report it and exit
    except Exception as e:
        errstr = f"Could not parse the configuration file " \
                 f"{configfilerun}: {e}"
        log.error(errstr)
        sys.exit(errstr)
    
    # try to get the configuration for data aggregation from the
    # corresponding configuration file
    try:
        configaggr = util.get_config_aggregate(configfileaggr)
    # if something went wrong, report it and exit
    except Exception as e:
        errstr = f"Could not parse the configuration file " \
                 f"{configfileaggr}: {e}"
        log.error(errstr)
        sys.exit(errstr)

    # get family of the protocol
    family = configrun["family"]

    # get the configuration for the output dataframes
    dfsconfig = configaggr["out_dfs"]
    
    # get the options to be used in writing the output dataframes
    dfsoptions = dfsconfig["options"]
    
    # whether to convert the scores to kcal/mol using the
    # conversion factors
    rescale = dfsconfig["convert_to_kcalmol"]

    # get the dataframe output filenames
    dfsoutnames = dfsconfig["out_names"]
    # get the names of the aggregated output files storing data for
    # all structures or per-structure
    mutaggr = dfsoutnames["out_aggregate"]
    mutstruct = dfsoutnames["out_structures"]
    # get the suffixes to be appended to each output file (all
    # structures or per-structure) concerning a single mutation
    oasuffix = dfsoutnames["out_suffix_aggregate"]
    ossuffix = dfsoutnames["out_suffix_structures"]



    ############################ AGGREGATION ##########################

    
    # create a list to store pending futures
    futures = []


    # create empty lists to store the dataframes for each mutation
    mutaggrdfs = []
    mutstructdfs = []

    
    # if the output directory does not exist, create it
    os.makedirs(outdir, exist_ok = True)
 

    # if the protocol is a cartddg protocol
    if family == "cartddg":
        
        # get the directory where the ΔΔG calculation step was run
        steprundir = configrun["steps"]["cartesian"]["wd"]
        
        # get the Rosetta options
        options = configrun["steps"]["cartesian"]["options"]
        
        # get the output file name
        outname = \
            options[util.get_option_key(options = options, \
                                        option = "ddgout")]

        # get the score function name
        scfname = \
            options[util.get_option_key(options = options, \
                                        option = "scfname")]
 

    # if the protocol is a flexddg protocol
    elif family == "flexddg":

        # get the directory where the ΔΔG calculation step was run
        steprundir = configrun["steps"]["flexddg"]["wd"]
        
        # get the Rosetta options
        options = configrun["steps"]["flexddg"]["options"]
        
        # get the RosettaScript options 
        rscriptoptions = \
            options[util.get_option_key(options = options, \
                                        option = "scriptvars")]

        # get the output file name
        outname = \
            rscriptoptions[\
                util.get_option_key(options = rscriptoptions, \
                                    option = "ddgdbfile")]

        # get the score function name
        scfname = \
            rscriptoptions[\
                util.get_option_key(options = rscriptoptions, \
                                    option = "scfname")]
        
        # get the number of backrub trials
        backrubntrials = \
            rscriptoptions[\
                util.get_option_key(options = rscriptoptions, \
                                    option = "backrubntrials")]
        
        # get the backrub trajectory stride
        backrubtrajstride = \
            rscriptoptions[\
                util.get_option_key(options = rscriptoptions, \
                                    option = "backrubtrajstride")]
    
        # get the number of structures generated
        nstruct = configrun["mutations"]["nstruct"]
        
        # format the structure names as strings
        structnums = [str(num) for num in range(1, nstruct + 1)]
        
        # compute the trajectory stride
        trajstride = int(backrubntrials) // int(backrubtrajstride)


    # get the list of contributions for the scoring function used
    listcontributions = configaggr["energy_contributions"][scfname]
    
    # get the conversion factor for the scoring function used
    convfact = configaggr["conversion_factors"][scfname]


    # if the specified running directory was "."
    if steprundir == ".":
        # the outputs were generated in the current working
        # directory 
        steprundirpath = rundir
    else:
        # the outputs were generated in a sub-directory
        steprundirpath = os.path.join(rundir, steprundir)
 

    # get info about the mutations
    try:
        mutinfo = client.submit(util.get_mutinfo, \
                                mutinfofile = mutinfofile).result()
    # if something went wrong, report it and exit
    except Exception as e:
        errstr = f"Could not load mutations' info from " \
                 f"{mutinfofile}: {e}"
        log.error(errstr)
        sys.exit(errstr)


    # for each mutation
    for i, (mutname, dirname, mutlabel, poslabel, mutr) \
        in mutinfo.iterrows():
        
        # get the mutation directory path
        mutpath = os.path.join(steprundirpath, dirname)

        # if the protocol is a cartddg protocol
        if family == "cartddg":
            
            # get the path to the output file
            ddgout = os.path.join(mutpath, outname)
            
            # try to parse the output file
            try:
                df = client.submit(\
                        aggregation.parse_output_cartddg, \
                        ddgout = ddgout, \
                        listcontributions = listcontributions, \
                        scfname = scfname)
            # if something went wrong, report it and continue
            except Exception as e:
                log.warning(f"Could not parse {ddgout}: {e}")
                continue
            
            # try to get the aggregated dataframes          
            try:
                dfs = \
                    client.submit(\
                        aggregation.aggregate_data_cartddg, \
                        df = df, \
                        listcontributions = listcontributions).result()
            # if something went wrong, report it and exit
            except Exception as e:
                errstr = f"Could not aggregate data for " \
                         f"{os.path.basename(mutpath)}: {e}"
                log.error(errstr)
                sys.exit(errstr)
        
        
        # if the protocol is a flexddg protocol
        elif family == "flexddg":
            
            # initialize an empty list to store
            # dataframes for all structures
            structdfs = []
            
            # for each structure
            for structnum in structnums:
                # structure path 
                structpath = os.path.join(mutpath, structnum)
                # path to the .db3 output file 
                db3out = os.path.join(structpath, outname)
                # try to create a dataframe from the .db3 output file
                try:
                    df = client.submit(\
                            aggregation.parse_output_flexddg, \
                            db3out = db3out, \
                            trajstride = trajstride, \
                            structnum = structnum, \
                            scfname = scfname)
                # if something went wrong, report it and continue
                except Exception as e:
                    log.warning(f"Could not parse {db3out}: {e}")
                    continue
                
                # append the dataframe to the list
                structdfs.append(df.result())

            # try to generate the aggregated dataframes            
            try:
                dfs = \
                    client.submit(\
                        aggregation.aggregate_data_flexddg, \
                        df = pd.concat(structdfs), \
                        listcontributions = listcontributions).result()
            # if something went wrong, report it and exit
            except Exception as e:
                errstr = f"Could not aggregate data for " \
                         f"{os.path.basename(mutpath)}: {e}"
                log.error(errstr)
                sys.exit(errstr)

        
        # separate the dataframes with the wild-type ΔG, the
        # mutant ΔG and the ΔΔG
        dgwt, dgmut, ddg = dfs


        # try to generate the aggregated and all-structures dataframes
        try:
            aggrdf, structdf = \
                client.submit(\
                    aggregation.generate_output_dataframes, \
                    dgwt = dgwt, \
                    dgmut = dgmut, \
                    ddg = ddg, \
                    mutation = mutname, \
                    mutlabel = mutlabel, \
                    poslabel = poslabel, \
                    rescale = rescale, \
                    listcontributions = listcontributions, \
                    convfact = convfact).result()
        # if something went wrong, report it and exit
        except Exception as e:
            errstr = f"Could not generate output dataframes: {e}"
            log.error(errstr)
            sys.exit(errstr)


        # save the aggregated dataframe
        aggrdfpath = os.path.join(outdir, mutlabel + oasuffix)
        futures.append(client.submit(aggrdf.to_csv, \
                                     aggrdfpath, \
                                     **dfsoptions))

        # save the all-structures dataframe
        structdfpath = os.path.join(outdir, mutlabel + ossuffix)
        futures.append(client.submit(structdf.to_csv, \
                                     structdfpath, \
                                     **dfsoptions))
        
        # add the dataframes to the lists of all-mutations dataframes
        mutaggrdfs.append(aggrdf)
        mutstructdfs.append(structdf)


    # aggregate the dataframes containing single mutations
    mutaggrdf = client.submit(pd.concat, \
                              mutaggrdfs).result()
    mutstructdf = client.submit(pd.concat, \
                                mutstructdfs).result()
    
    # save the dataframes with data for all the mutations
    mutaggrdfpath = os.path.join(outdir, mutaggr)
    futures.append(client.submit(mutaggrdf.to_csv, \
                                 mutaggrdfpath, \
                                 **dfsoptions))

    mutstructdfpath = os.path.join(outdir, mutstruct)
    futures.append(client.submit(mutstructdf.to_csv, \
                                 mutstructdfpath, \
                                 **dfsoptions))

    # gather pending futures
    client.gather(futures)


if __name__ == "__main__":
    main()