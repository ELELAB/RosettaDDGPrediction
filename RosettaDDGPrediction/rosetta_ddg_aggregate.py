#!/usr/bin/env python
# -*- Mode: python; tab-width: 4; indent-tabs-mode:nil; coding:utf-8 -*-

#    rosetta_ddg_aggregate.py
#
#    Aggregate data generated by running Rosetta protocols for the
#    calculation of the ΔΔG of stability/binding upon mutation.
#
#    Copyright (C) 2022 Valentina Sora 
#                       <sora.valentina1@gmail.com>
#                       Matteo Tiberti 
#                       <matteo.tiberti@gmail.com> 
#                       Elena Papaleo
#                       <elenap@cancer.dk>
#
#    This program is free software: you can redistribute it and/or
#    modify it under the terms of the GNU General Public License as
#    published by the Free Software Foundation, either version 3 of
#    the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public
#    License along with this program. 
#    If not, see <http://www.gnu.org/licenses/>.



# Standard libary
import argparse
import logging as log
import os
import os.path
import sys
# Third-party packages
import dask
from distributed import Client, LocalCluster
import pandas as pd
# RosettaDDGProtocols
from . import aggregation 
from .defaults import (
    CONFIG_AGGR_DIR,
    CONFIG_AGGR_FILE,
    CONFIG_RUN_DIR,
    CONFIG_SETTINGS_DIR,
)
from . import util


def main():



    ######################### ARGUMENT PARSER #########################



    # Create argument parser
    parser = argparse.ArgumentParser()

    cr_help = f"Configuration file of the protocol that was " \
              f"run. If it is a name without extension, it is " \
              f"assumed to be the name of a YAML file in " \
              f"{CONFIG_RUN_DIR}."
    parser.add_argument("-cr", "--configfile-run",
                        type = str,
                        required = True,
                        help = cr_help)

    cs_help = \
        f"Configuration file containing settings to be used for " \
        f"the run. If it is a name without extension, it is assumed " \
        f"to be the name of a YAML file in {CONFIG_SETTINGS_DIR}. "
    parser.add_argument("-cs", "--configfile-settings",
                        type = str,
                        required = True,
                        help = cs_help)

    ca_help = f"Configuration file for data aggregation. " \
              f"If it is a name without extension, it is assumed " \
              f"to be the name of a YAML file in {CONFIG_AGGR_DIR}. " \
              f"Default is {CONFIG_AGGR_FILE}."
    parser.add_argument("-ca", "--configfile-aggregate",
                        type = str,
                        default = CONFIG_AGGR_FILE,
                        help = ca_help)

    d_help = "Directory where the protocol was run. " \
             "Default is the current working directory."
    parser.add_argument("-d", "--running-dir",
                        type = str,
                        default = os.getcwd(),
                        help = d_help)

    od_help = \
        "Directory where to store the aggregated data. " \
        "Default is the current working directory."
    parser.add_argument("-od", "--output-dir",
                        type = str,
                        default = os.getcwd(),
                        help = od_help)

    mf_help = "File with info about the mutations (it " \
              "is created when running)." 
    parser.add_argument("-mf", "--mutinfofile",
                        type = str,
                        help = mf_help)

    n_help = \
        "Number of processes to be started in parallel. " \
        "Default is one process (no parallelization)."
    parser.add_argument("-n", "--nproc",
                        type = int,
                        default = 1,
                        help = n_help)

    # Parse the arguments
    args = parser.parse_args()
    
    # Configuration files
    config_file_run = args.configfile_run
    config_file_settings = args.configfile_settings
    config_file_aggr = args.configfile_aggregate
    
    # Directories
    run_dir = args.running_dir
    out_dir = args.output_dir
    
    # Others
    mutinfo_file = util.get_abspath(args.mutinfofile)
    n_proc = args.nproc



    ############################## CLIENT #############################



    # Try to get the run settings from the default YAML file
    try:
        settings = util.get_config_settings(config_file_settings)
    
    # If something went wrong, report it and exit
    except Exception as e:
        errstr = f"Could not parse the configuration file " \
                 f"{config_file_settings}: {e}"
        log.error(errstr)
        sys.exit(errstr)

    # Create the local cluster
    cluster = LocalCluster(n_workers = n_proc,
                           **settings["localcluster"])
    
    # Open the client from the cluster
    client = Client(cluster)



    ########################## CONFIGURATION ##########################



    # Try to get the configuration of the run from the
    # corresponding configuration file
    try:
        
        config_run = util.get_config_run(config_file_run)
    
    # If something went wrong, report it and exit
    except Exception as e:
        
        errstr = f"Could not parse the configuration file " \
                 f"{config_file_run}: {e}"
        log.error(errstr)
        sys.exit(errstr)
    
    # Try to get the configuration for data aggregation from the
    # corresponding configuration file
    try:
        
        config_aggr = util.get_config_aggregate(config_file_aggr)
    
    # If something went wrong, report it and exit
    except Exception as e:
        
        errstr = f"Could not parse the configuration file " \
                 f"{config_file_aggr}: {e}"
        log.error(errstr)
        sys.exit(errstr)

    # Get family of the protocol
    family = config_run["family"]

    # Get the configuration for the output data frames
    dfs_config = config_aggr["out_dfs"]
    
    # Get the options to be used in writing the output data frames
    dfs_options = dfs_config["options"]
    
    # Whether to convert the scores to kcal/mol using the
    # conversion factors
    rescale = dfs_config["convert_to_kcalmol"]

    # Get the data frame output filenames
    dfs_out_names = dfs_config["out_names"]
    
    # Get the names of the aggregated output files storing data for
    # all structures or per-structure
    mut_aggr = dfs_out_names["out_aggregate"]
    mut_struct = dfs_out_names["out_structures"]
    
    # Get the suffixes to be appended to each output file (all
    # structures or per-structure) concerning a single mutation
    oa_suffix = dfs_out_names["out_suffix_aggregate"]
    os_suffix = dfs_out_names["out_suffix_structures"]



    ############################ AGGREGATION ##########################



    # Create a list to store pending futures
    futures = []

    # Create empty lists to store the data frames for each mutation
    mut_aggr_dfs = []
    mut_struct_dfs = []
    
    # If the output directory does not exist, create it
    os.makedirs(out_dir, exist_ok = True)


    # If the protocol is a cartddg protocol
    if family == "cartddg":
        
        # Get the directory where the ΔΔG calculation step was run
        step_run_dir = config_run["steps"]["cartesian"]["wd"]
        
        # Get the Rosetta options
        options = config_run["steps"]["cartesian"]["options"]
        
        # Get the output file name
        out_name = \
            options[util.get_option_key(options = options,
                                        option = "ddg_out")]

        # Get the score function name
        scf_name = \
            options[util.get_option_key(options = options,
                                        option = "scf_name")]


    # If the protocol is a flexddg protocol
    elif family == "flexddg":

        # Get the directory where the ΔΔG calculation step was run
        step_run_dir = config_run["steps"]["flexddg"]["wd"]
        
        # Get the Rosetta options
        options = config_run["steps"]["flexddg"]["options"]
        
        # Get the RosettaScript options 
        r_script_options = \
            options[util.get_option_key(options = options,
                                        option = "script_vars")]

        # Get the output file name
        out_name = \
            r_script_options[\
                util.get_option_key(options = r_script_options,
                                    option = "ddg_db_file")]

        # Get the score function name
        scf_name = \
            r_script_options[\
                util.get_option_key(options = r_script_options,
                                    option = "scf_name")]
        
        # Get the number of backrub trials
        backrub_n_trials = \
            r_script_options[\
                util.get_option_key(options = r_script_options,
                                    option = "backrub_n_trials")]
        
        # Get the backrub trajectory stride
        backrub_traj_stride = \
            r_script_options[\
                util.get_option_key(options = r_script_options,
                                    option = "backrub_traj_stride")]
    
        # Get the number of structures generated
        n_struct = config_run["mutations"]["nstruct"]
        
        # Format the structure names as strings
        struct_nums = [str(num) for num in range(1, n_struct + 1)]
        
        # Compute the trajectory stride
        traj_stride = int(backrub_n_trials) // int(backrub_traj_stride)


    # Get the list of contributions for the scoring function used
    list_contributions = config_aggr["energy_contributions"][scf_name]
    
    # Get the conversion factor for the scoring function used
    conv_fact = config_aggr["conversion_factors"][scf_name]


    # If the specified running directory was "."
    if step_run_dir == ".":
        
        # The outputs were generated in the current working
        # directory 
        step_run_dir_path = run_dir
    
    else:
        
        # The outputs were generated in a sub-directory
        step_run_dir_path = os.path.join(run_dir, step_run_dir)
 

    # Get info about the mutations
    try:
        
        mutinfo = client.submit(util.get_mutinfo,
                                mutinfo_file = mutinfo_file).result()
    
    # If something went wrong, report it and exit
    except Exception as e:
        
        errstr = f"Could not load mutations' info from " \
                 f"{mutinfo_file}: {e}"
        log.error(errstr)
        sys.exit(errstr)


    # For each mutation
    for i, (mut_name, dir_name, mut_label, pos_label, mutr) \
        in mutinfo.iterrows():
        
        # Get the mutation directory path
        mut_path = os.path.join(step_run_dir_path, dir_name)

        # If the protocol is a cartddg protocol
        if family == "cartddg":
            
            # Get the path to the output file
            ddg_out = os.path.join(mut_path, out_name)
            
            # Try to parse the output file
            try:
                
                df = client.submit(\
                        aggregation.parse_output_cartddg,
                        ddg_out = ddg_out,
                        list_contributions = list_contributions,
                        scf_name = scf_name)
            
            # If something went wrong, report it and continue
            except Exception as e:
                
                log.warning(f"Could not parse {ddg_out}: {e}")
                continue
            
            # Try to get the aggregated dataframes          
            try:
                
                dfs = \
                    client.submit(\
                        aggregation.aggregate_data_cartddg,
                        df = df,
                        list_contributions = list_contributions).result()
            
            # If something went wrong, report it and exit
            except Exception as e:
                
                errstr = f"Could not aggregate data for " \
                         f"{os.path.basename(mut_path)}: {e}"
                log.error(errstr)
                sys.exit(errstr)
        
        
        # If the protocol is a flexddg protocol
        elif family == "flexddg":
            
            # Initialize an empty list to store
            # dataframes for all structures
            struct_dfs = []
            
            # For each structure
            for struct_num in struct_nums:
                
                # Structure path 
                struct_path = os.path.join(mut_path, struct_num)
                
                # Path to the .db3 output file 
                db3_out = os.path.join(struct_path, out_name)
                
                # Try to create a dataframe from the .db3 output file
                try:
                    
                    df = client.submit(\
                            aggregation.parse_output_flexddg,
                            db3_out = db3_out,
                            traj_stride = traj_stride,
                            struct_num = struct_num,
                            scf_name = scf_name)
                
                # If something went wrong, report it and continue
                except Exception as e:
                    
                    log.warning(f"Could not parse {db3_out}: {e}")
                    continue
                
                # Append the dataframe to the list
                struct_dfs.append(df.result())

            # Try to generate the aggregated dataframes            
            try:
                
                dfs = \
                    client.submit(\
                        aggregation.aggregate_data_flexddg,
                        df = pd.concat(struct_dfs),
                        list_contributions = list_contributions).result()
            
            # If something went wrong, report it and exit
            except Exception as e:
                
                errstr = f"Could not aggregate data for " \
                         f"{os.path.basename(mut_path)}: {e}"
                log.error(errstr)
                sys.exit(errstr)

        
        # Separate the dataframes with the wild-type ΔG, the
        # mutant ΔG and the ΔΔG
        dg_wt, dg_mut, ddg = dfs

        # Try to generate the aggregated and all-structures dataframes
        try:
            
            aggr_df, struct_df = \
                client.submit(\
                    aggregation.generate_output_dataframes,
                    dg_wt = dg_wt,
                    dg_mut = dg_mut,
                    ddg = ddg,
                    mutation = mut_name,
                    mut_label = mut_label,
                    pos_label = pos_label,
                    rescale = rescale,
                    list_contributions = list_contributions,
                    conv_fact = conv_fact).result()
        
        # If something went wrong, report it and exit
        except Exception as e:
            
            errstr = f"Could not generate output dataframes: {e}"
            log.error(errstr)
            sys.exit(errstr)


        # Save the aggregated dataframe
        aggr_df_path = os.path.join(out_dir, mut_label + oa_suffix)
        futures.append(client.submit(aggr_df.to_csv,
                                     aggr_df_path,
                                     **dfs_options))

        # Save the all-structures dataframe
        struct_df_path = os.path.join(out_dir, mut_label + os_suffix)
        futures.append(client.submit(struct_df.to_csv,
                                     struct_df_path,
                                     **dfs_options))
        
        # Add the dataframes to the lists of all-mutations dataframes
        mut_aggr_dfs.append(aggr_df)
        mut_struct_dfs.append(struct_df)


    # Aggregate the dataframes containing single mutations
    mut_aggr_df = client.submit(pd.concat,
                                mut_aggr_dfs).result()
    mut_struct_df = client.submit(pd.concat,
                                  mut_struct_dfs).result()
    
    # Save the dataframes with data for all the mutations
    mut_aggr_df_path = os.path.join(out_dir, mut_aggr)
    futures.append(client.submit(mut_aggr_df.to_csv,
                                 mut_aggr_df_path,
                                 **dfs_options))

    mut_struct_df_path = os.path.join(out_dir, mut_struct)
    futures.append(client.submit(mut_struct_df.to_csv, \
                                 mut_struct_df_path, \
                                 **dfs_options))

    # Gather pending futures
    client.gather(futures)


if __name__ == "__main__":
    main()